import asyncio
import logging
import random
import re
import uuid
from typing import List, Dict, Optional
from datetime import datetime, timedelta
import whois
import requests
from curl_cffi.requests import AsyncSession
from bs4 import BeautifulSoup
from app.core.config import settings
from app.database import SessionLocal
from app.models.domain import Domain

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

OPENPAGERANK_API_KEY = settings.OPENPAGERANK_API_KEY if hasattr(settings, 'OPENPAGERANK_API_KEY') else "w00wkkkwo4c4sws4swggkswk8oksggsccck0go84"

class DomainScanner:
    def __init__(self, mode: str = "expireddomains"):
        """
        åˆå§‹åŒ–æ‰«æå™¨
        :param mode: æ‰«ææ¨¡å¼ï¼Œé»˜è®¤ "expireddomains"
        """
        self.db = SessionLocal()
        self.mode = mode

    def verify_expiry_date_via_whois(self, domain_name: str) -> Dict:
        """
        éªŒè¯åŸŸåçš„çœŸå®åˆ°æœŸæ—¶é—´
        è¿”å›: {'real_expiry': datetime, 'is_expired': bool, 'is_valid': bool}
        """
        try:
            w = whois.whois(domain_name)
            
            # å¤„ç† whois è¿”å›çš„æ—¥æœŸå¯èƒ½æ˜¯åˆ—è¡¨çš„æƒ…å†µ
            expiry_date = w.expiration_date
            if isinstance(expiry_date, list):
                expiry_date = expiry_date[0]
                
            if not expiry_date:
                return {'real_expiry': None, 'is_expired': False, 'is_valid': False}
                
            now = datetime.now()
            is_expired = expiry_date < now
            
            return {
                'real_expiry': expiry_date,
                'is_expired': is_expired,
                'is_valid': True
            }
        except Exception as e:
            return {'real_expiry': None, 'is_expired': False, 'is_valid': False}

    def extract_number(self, text: str) -> int:
        """æ­£åˆ™æå–æ•°å­—ï¼Œå¤„ç† 1.8Kã€1,992 ç­‰æ ¼å¼"""
        if not text:
            return 0
        
        match = re.search(r'(\d+(?:\.\d+)?)\s*K', text.upper())
        if match:
            return int(float(match.group(1)) * 1000)
        
        match = re.search(r'(\d[\d,]*)', text)
        if match:
            return int(match.group(1).replace(',', ''))
        
        return 0

    def batch_get_pagerank(self, domain_names: List[str]) -> Dict[str, int]:
        """æ‰¹é‡è·å– DA åˆ†æ•°ï¼ˆé€šè¿‡ OpenPageRank APIï¼‰"""
        if not domain_names:
            return {}
        
        results = {}
        batch_size = 100
        
        logger.info(f"ğŸ” å¼€å§‹æ‰¹é‡è·å– {len(domain_names)} ä¸ªåŸŸåçš„ DA åˆ†æ•°...")
        
        for i in range(0, len(domain_names), batch_size):
            batch = domain_names[i:i+batch_size]
            
            params = {f"domains[{j}]": domain for j, domain in enumerate(batch)}
            
            try:
                response = requests.get(
                    "https://openpagerank.com/api/v1.0/getPageRank",
                    params=params,
                    headers={'API-OPR': OPENPAGERANK_API_KEY},
                    timeout=10
                )
                
                data = response.json()
                
                if data.get('status_code') == 200 and data.get('response'):
                    for item in data['response']:
                        domain = item['domain']
                        page_rank = item.get('page_rank_decimal', 0)
                        da_score = int(page_rank * 10)
                        results[domain] = da_score
                        logger.info(f"  âœ… {domain} â†’ DA: {da_score}")
                    
                    logger.info(f"âœ… æ‰¹æ¬¡å®Œæˆ: æˆåŠŸè·å– {len(batch)} ä¸ªåŸŸåçš„ DA")
                else:
                    logger.warning(f"âš ï¸ OpenPageRank API é”™è¯¯: {data}")
                    for domain in batch:
                        results[domain] = 0
                
                import time
                time.sleep(1)
                
            except Exception as e:
                logger.error(f"âŒ æ‰¹é‡è·å– DA å¤±è´¥: {e}")
                for domain in batch:
                    results[domain] = 0
        
        return results

    async def fetch_single_page(self, page: int, retries=3) -> List[Dict]:
        """æŠ“å–å•é¡µæ•°æ®ï¼ˆå¸¦é‡è¯•ï¼‰"""
        url = "https://member.expireddomains.net/domains/expiredcom/"
        cookies = {
            "s_id": settings.EXPIRED_DOMAINS_COOKIE
        }
        headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
            "Referer": "https://member.expireddomains.net/domains/expiredcom/"
        }

        # è®¡ç®—åˆ†é¡µå‚æ•°
        start = (page - 1) * 25
        params = {
            "start": start,
            "flimit": 25,
            "fwhois": "1",
            "fmarket": "0",
            "flast24": "1"
        }

        for attempt in range(retries):
            try:
                try:
                    async with AsyncSession() as session:
                        resp = await session.get(url, params=params, cookies=cookies, headers=headers, timeout=30)
                        
                        if resp.status_code != 200:
                            logger.warning(f"Page {page} failed with status {resp.status_code}")
                            continue
                            
                        content = resp.text
                except RuntimeError:
                    pass
                except Exception as e:
                    logger.error(f"Request error on page {page}: {e}")
                    continue

                domains = []
                soup = BeautifulSoup(content, 'html.parser')
                rows = soup.select('table.base1 tbody tr')
                
                for row in rows:
                    cols = row.find_all('td')
                    if len(cols) < 2:
                        continue
                        
                    domain_name = cols[0].get_text(strip=True)
                    
                    bl = 0
                    try:
                        bl_text = cols[2].get_text(strip=True)
                        bl = self.extract_number(bl_text)
                    except:
                        pass

                    domains.append({
                        "name": domain_name,
                        "backlinks": bl,
                        "da_score": 0,
                        "status": "pending"
                    })
                
                logger.info(f"âœ… ç¬¬ {page} é¡µï¼šæˆåŠŸè§£æ {len(domains)} ä¸ªåŸŸå")
                return domains

            except Exception as e:
                logger.error(f"Attempt {attempt+1} failed for page {page}: {str(e)}")
                await asyncio.sleep(2)
        
        return []

    async def fetch_expireddomains_multi_pages(self, pages=4) -> List[Dict]:
        """å¹¶å‘æŠ“å–å¤šé¡µ"""
        logger.info(f"ğŸš€ å¼€å§‹æŠ“å–å‰ {pages} é¡µï¼ˆå…±çº¦ {pages*25} ä¸ªåŸŸåï¼‰...")
        tasks = [self.fetch_single_page(page) for page in range(1, pages + 1)]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        all_domains = []
        for res in results:
            if isinstance(res, list):
                all_domains.extend(res)
        
        logger.info(f"âœ… å…±æŠ“å– {len(all_domains)} ä¸ªåŸŸå")
        
        # ğŸ”¥ æ–°å¢ï¼šæŠ“å–åˆ°çš„åŸŸåç«‹å³å…¨éƒ¨å­˜å…¥æ•°æ®åº“
        if all_domains:
            logger.info(f"ğŸ’¾ å¼€å§‹å°† {len(all_domains)} ä¸ªæŠ“å–åŸŸåå­˜å…¥æ•°æ®åº“...")
            saved_count = 0
            for d in all_domains:
                try:
                    exists = self.db.query(Domain).filter(Domain.name == d['name']).first()
                    if not exists:
                        new_domain = Domain(
                            name=d['name'],
                            da_score=d.get('da_score', 0),
                            backlinks=d.get('backlinks', 0),
                            status='scraped',  # æ ‡è®°ä¸ºçˆ¬å–çš„
                            drop_date=None
                        )
                        self.db.add(new_domain)
                        saved_count += 1
                except Exception as e:
                    logger.error(f"ä¿å­˜åŸŸå {d['name']} å¤±è´¥: {e}")
                    continue
            
            try:
                self.db.commit()
                logger.info(f"âœ… æˆåŠŸå°† {saved_count} ä¸ªæŠ“å–åŸŸåå­˜å…¥æ•°æ®åº“")
            except Exception as e:
                self.db.rollback()
                logger.error(f"æ‰¹é‡ä¿å­˜å¤±è´¥: {e}")
        
        return all_domains

    def generate_mock_domains(self, count=20) -> List[Dict]:
        """Bå±‚ï¼šç”Ÿæˆæ¨¡æ‹Ÿçš„é«˜è´¨é‡åŸŸåï¼ˆç¡®ä¿å”¯ä¸€ï¼‰"""
        logger.info(f"âš ï¸ [B å±‚] è§¦å‘é™çº§ï¼šç”Ÿæˆ {count} ä¸ªæ¨¡æ‹ŸåŸŸå")
        
        TECH_KEYWORDS = ["ai", "gpt", "gemini", "claude", "quantum", "neural", "crypto", "defi", "metaverse"]
        PREFIXES = ["super", "ultra", "mega", "next", "smart", "auto", "hyper"]
        SUFFIXES = ["hub", "lab", "flow", "cloud", "stack", "forge", "sphere"]
        
        mock_domains = []
        
        for i in range(count):
            # ä½¿ç”¨ UUID ç¡®ä¿å”¯ä¸€æ€§
            unique_suffix = str(uuid.uuid4())[:8]
            
            pattern = random.choice([
                f"{random.choice(TECH_KEYWORDS)}-{unique_suffix}",
                f"{random.choice(PREFIXES)}{random.choice(TECH_KEYWORDS)}-{unique_suffix}",
                f"{random.choice(TECH_KEYWORDS)}{random.choice(SUFFIXES)}-{unique_suffix}"
            ])
            
            tld = random.choice([".com", ".ai", ".io", ".net"])
            d_name = pattern + tld
            
            mock_domains.append({
                "name": d_name,
                "da_score": random.randint(25, 65),
                "backlinks": random.randint(50, 500),
                "status": "mock",  # æ ‡è®°ä¸ºæ¨¡æ‹Ÿçš„
                "drop_date": datetime.now() + timedelta(days=random.randint(1, 30))
            })
            
            logger.info(f"  ğŸ­ ç”Ÿæˆ: {d_name} (DA: {mock_domains[-1]['da_score']})")
            
        return mock_domains

    async def scan(self):
        """ä¸»æ‰«æé€»è¾‘ï¼šA -> B é™çº§"""
        logger.info("ğŸš€ å¼€å§‹äºŒå±‚é™çº§æ‰«æ...")
        
        final_results = []
        
        # --- A å±‚ï¼šçœŸå®çˆ¬è™«ï¼ˆ100ä¸ªåŸŸåå…¨å­˜åº“ï¼‰---
        logger.info("ğŸ•·ï¸ [A å±‚] æŠ“å– ExpiredDomains.net")
        raw_domains = await self.fetch_expireddomains_multi_pages(pages=4)
        
        if raw_domains:
            # 1. æ‰¹é‡è·å–çœŸå® DA åˆ†æ•°ï¼ˆä»…å¯¹ Top 20ï¼‰
            logger.info(f"ğŸ” å¼€å§‹è·å– Top 20 çš„ DA åˆ†æ•°...")
            top_20 = sorted(raw_domains, key=lambda x: x.get('backlinks', 0), reverse=True)[:20]
            domain_names = [d['name'] for d in top_20]
            da_scores = self.batch_get_pagerank(domain_names)
            
            for d in top_20:
                d['da_score'] = da_scores.get(d['name'], 0)
                
            # 2. æŒ‰ DA æ’åºå– Top 5
            top_domains = sorted(top_20, key=lambda x: x['da_score'], reverse=True)[:5]
            
            # 3. WHOIS éªŒè¯
            logger.info("ğŸ” å¯¹ Top 5 è¿›è¡Œ WHOIS éªŒè¯...")
            valid_a_domains = []
            for d in top_domains:
                logger.info(f"Checking {d['name']}...")
                verify_res = self.verify_expiry_date_via_whois(d['name'])
                
                if verify_res['is_expired']:
                    logger.info(f"âœ… éªŒè¯é€šè¿‡: {d['name']} (è¿‡æœŸæ—¥: {verify_res['real_expiry']})")
                    d['drop_date'] = verify_res['real_expiry']
                    d['status'] = 'expired_confirmed'
                    valid_a_domains.append(d)
                else:
                    logger.info(f"âŒ å·²ç»­è´¹: {d['name']} (åˆ°æœŸæ—¥: {verify_res.get('real_expiry')})")
            
            final_results.extend(valid_a_domains)
        
        # --- B å±‚ï¼šæ¨¡æ‹Ÿæ•°æ®ï¼ˆå¦‚æœ A å±‚ç»“æœä¸è¶³ï¼‰---
        if len(final_results) < 2:
            logger.info("âš ï¸ A å±‚æœ‰æ•ˆæ•°æ®ä¸è¶³ï¼Œå¯åŠ¨ B å±‚è¡¥ä½...")
            mock_count = 8 if len(final_results) == 0 else (5 - len(final_results))
            mock_data = self.generate_mock_domains(count=mock_count)
            final_results.extend(mock_data)

        # è¿”å›å­—å…¸æ ¼å¼ï¼Œç”± endpoints.py ç»Ÿä¸€å…¥åº“
        logger.info(f"âœ… æ‰«æå®Œæˆï¼Œè¿”å› {len(final_results)} ä¸ªåŸŸåï¼ˆå¾…å±•ç¤ºï¼‰")
        return {
            "all_domains": final_results,
            "top_5": final_results[:5]
        }