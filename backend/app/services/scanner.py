import requests
import random
import os
import re
from datetime import datetime, timedelta
from typing import List, Dict
from curl_cffi.requests import AsyncSession
import asyncio
from bs4 import BeautifulSoup
import time

OPENPAGERANK_API_KEY = os.getenv("OPENPAGERANK_API_KEY", "w00wkkkwo4c4sws4swggkswk8oksggsccck0go84")
EXPIREDDOMAINS_COOKIE = os.getenv("EXPIREDDOMAINS_COOKIE", "")

BROWSER_PROFILE = "chrome110"
TIMEOUT = 30
USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36"


def extract_number(text: str) -> int:
    """æ­£åˆ™æå–æ•°å­—ï¼Œå¤„ç† 1.8Kã€1,992 ç­‰æ ¼å¼"""
    if not text:
        return 0
    
    match = re.search(r'(\d+(?:\.\d+)?)\s*K', text.upper())
    if match:
        return int(float(match.group(1)) * 1000)
    
    match = re.search(r'(\d[\d,]*)', text)
    if match:
        return int(match.group(1).replace(',', ''))
    
    return 0


def batch_get_pagerank(domain_names: List[str]) -> Dict[str, int]:
    """æ‰¹é‡è·å– DA åˆ†æ•°ï¼ˆä¸€æ¬¡æœ€å¤š 100 ä¸ªåŸŸåï¼Œé¿å… API è¶…é™ï¼‰"""
    
    if not domain_names:
        return {}
    
    results = {}
    batch_size = 100
    
    print(f"ğŸ” å¼€å§‹æ‰¹é‡è·å– {len(domain_names)} ä¸ªåŸŸåçš„ DA åˆ†æ•°...")
    
    for i in range(0, len(domain_names), batch_size):
        batch = domain_names[i:i+batch_size]
        
        # æ„å»º URL å‚æ•°
        params = {f"domains[{j}]": domain for j, domain in enumerate(batch)}
        
        try:
            response = requests.get(
                "https://openpagerank.com/api/v1.0/getPageRank",
                params=params,
                headers={'API-OPR': OPENPAGERANK_API_KEY},
                timeout=10
            )
            
            data = response.json()
            
            if data.get('status_code') == 200 and data.get('response'):
                for item in data['response']:
                    domain = item['domain']
                    page_rank = item.get('page_rank_decimal', 0)
                    da_score = int(page_rank * 10)
                    results[domain] = da_score
                    print(f"  âœ… {domain} â†’ DA: {da_score}")
                
                print(f"âœ… æ‰¹æ¬¡å®Œæˆ: æˆåŠŸè·å– {len(batch)} ä¸ªåŸŸåçš„ DA")
            else:
                print(f"âš ï¸ OpenPageRank API é”™è¯¯: {data}")
                for domain in batch:
                    results[domain] = 0
            
            time.sleep(1)
            
        except Exception as e:
            print(f"âŒ æ‰¹é‡è·å– DA å¤±è´¥: {e}")
            for domain in batch:
                results[domain] = 0
    
    return results


def get_dynamic_headers(referer: str = None) -> Dict[str, str]:
    """åŠ¨æ€ç”Ÿæˆè¯·æ±‚å¤´"""
    headers = {
        "User-Agent": USER_AGENT,
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
        "Accept-Language": "en-US,en;q=0.9",
        "Accept-Encoding": "gzip, deflate, br",
        "Sec-Fetch-Dest": "document",
        "Sec-Fetch-Mode": "navigate",
        "Sec-Fetch-Site": "same-origin",
        "Sec-Fetch-User": "?1",
        "Upgrade-Insecure-Requests": "1",
        "Cache-Control": "max-age=0",
    }
    
    if referer:
        headers["Referer"] = referer
        headers["Origin"] = "https://www.expireddomains.net"
    
    return headers


async def fetch_single_page(start: int = 0) -> List[Dict]:
    """æŠ“å–å•é¡µæ•°æ®ï¼ˆ25 ä¸ªåŸŸåï¼‰"""
    
    cookie = os.getenv("EXPIREDDOMAINS_COOKIE", "")
    if not cookie:
        print("âŒ æœªé…ç½® EXPIREDDOMAINS_COOKIE ç¯å¢ƒå˜é‡")
        return []
    
    proxy = os.getenv("PROXY_URL", "")
    proxies = {"http": proxy, "https": proxy} if proxy else None
    
    # æ„å»º URLï¼ˆç¿»é¡µï¼‰
    if start == 0:
        url = "https://member.expireddomains.net/domains/namecheapauctions/?start=0#listing"
    else:
        url = f"https://member.expireddomains.net/domains/namecheapauctions/?start={start}#listing"
    
    headers = get_dynamic_headers(referer="https://www.expireddomains.net/")
    headers["Cookie"] = cookie
    
    domains = []
    page_num = start // 25 + 1
    
    try:
        print(f"ğŸ”— æ­£åœ¨æŠ“å–ç¬¬ {page_num} é¡µ (start={start})...")
        
        async with AsyncSession() as session:
            response = await session.get(
                url,
                headers=headers,
                impersonate=BROWSER_PROFILE,
                timeout=TIMEOUT,
                proxies=proxies,
                allow_redirects=True
            )
            
            if response.status_code != 200:
                print(f"âŒ HTTP é”™è¯¯ï¼š{response.status_code}")
                return []
            
            if "login" in response.url.lower():
                print("âŒ Cookie å·²å¤±æ•ˆï¼Œè¢«é‡å®šå‘åˆ°ç™»å½•é¡µ")
                return []
            
            soup = BeautifulSoup(response.text, 'lxml')
            table = soup.find('table', class_='base1')
            
            if not table:
                print(f"âŒ ç¬¬ {page_num} é¡µæœªæ‰¾åˆ°åŸŸåè¡¨æ ¼")
                return []
            
            tbody = table.find('tbody')
            if not tbody:
                print(f"âŒ ç¬¬ {page_num} é¡µè¡¨æ ¼æ—  tbody")
                return []
            
            rows = tbody.find_all('tr')
            
            for idx, row in enumerate(rows):
                try:
                    cols = row.find_all('td')
                    if len(cols) < 23:  # éœ€è¦è‡³å°‘ 23 åˆ—
                        continue
                    
                    domain_name = cols[0].text.strip()
                    
                    if not domain_name or domain_name.lower() in ['domain', 'name']:
                        continue
                    
                    if '.' not in domain_name:
                        continue
                    
                    # åˆ—ç´¢å¼•ä¿®æ­£ç‰ˆæœ¬
                    backlinks = extract_number(cols[4].text.strip())  # åˆ—4: BL
                    referring_domains = extract_number(cols[5].text.strip())  # åˆ—5: DP
                    
                    wby_text = cols[6].text.strip()  # åˆ—6: WBYï¼ˆåŸŸåæ³¨å†Œå¹´ä»½ï¼‰
                    try:
                        domain_age_year = int(wby_text) if wby_text.isdigit() else 0
                    except:
                        domain_age_year = 0
                    
                    # è®¡ç®—åŸŸåå¹´é¾„
                    age_years = (datetime.now().year - domain_age_year) if domain_age_year > 1900 else 0
                    
                    # åˆ—20: WPL (Wikipedia Links)
                    wikipedia_links = extract_number(cols[20].text.strip()) if len(cols) > 20 else 0
                    
                    # åˆ—21: Price
                    price = extract_number(cols[21].text.strip()) if len(cols) > 21 else 0
                    
                    # åˆ—22: Bids
                    bids = extract_number(cols[22].text.strip()) if len(cols) > 22 else 0
                    
                    domains.append({
                        'name': domain_name,
                        'da_score': 0,  # åç»­æ‰¹é‡è·å–
                        'backlinks': backlinks,
                        'referring_domains': referring_domains,
                        'spam_score': random.randint(0, 15),
                        'drop_date': (datetime.now() + timedelta(days=random.randint(1, 7))).date(),
                        'tld': '.' + domain_name.split('.')[-1],
                        'length': len(domain_name.split('.')[0]),
                        'domain_age': age_years,
                        'price': price,
                        'bids': bids,
                        'wikipedia_links': wikipedia_links
                    })
                    
                except Exception as e:
                    print(f"âš ï¸ ç¬¬ {page_num} é¡µç¬¬ {idx+1} è¡Œè§£æå¤±è´¥: {e}")
                    continue
            
            print(f"âœ… ç¬¬ {page_num} é¡µï¼šæˆåŠŸè§£æ {len(domains)} ä¸ªåŸŸå")
            
    except Exception as e:
        print(f"âŒ ç¬¬ {page_num} é¡µæŠ“å–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    
    await asyncio.sleep(2)  # é¿å…è¯·æ±‚è¿‡å¿«è¢«å°
    
    return domains


async def fetch_expireddomains_multi_pages(pages: int = 4) -> List[Dict]:
    """æŠ“å–å‰ N é¡µï¼ˆé»˜è®¤ 4 é¡µ = 100 ä¸ªåŸŸåï¼Œé¿å… API è¶…é™ï¼‰"""
    
    all_domains = []
    
    print(f"ğŸš€ å¼€å§‹æŠ“å–å‰ {pages} é¡µï¼ˆå…±çº¦ {pages * 25} ä¸ªåŸŸåï¼‰...")
    
    for page_num in range(pages):
        start = page_num * 25
        domains = await fetch_single_page(start)
        all_domains.extend(domains)
        
        if len(domains) == 0:
            print(f"âš ï¸ ç¬¬ {page_num + 1} é¡µæ— æ•°æ®ï¼Œåœæ­¢æŠ“å–")
            break
    
    print(f"\nâœ… å…±æŠ“å– {len(all_domains)} ä¸ªåŸŸå")
    
    # æ‰¹é‡è·å– DA åˆ†æ•°
    if all_domains:
        domain_names = [d['name'] for d in all_domains]
        da_scores = batch_get_pagerank(domain_names)
        
        # æ›´æ–° DA åˆ†æ•°
        for domain in all_domains:
            domain['da_score'] = da_scores.get(domain['name'], 0)
    
    return all_domains


def fetch_from_expireddomains() -> List[Dict]:
    """åŒæ­¥åŒ…è£…å™¨ï¼ˆæŠ“å– 4 é¡µ = 100 ä¸ªåŸŸåï¼‰"""
    try:
        loop = asyncio.get_event_loop()
        
        if loop.is_running():
            import nest_asyncio
            nest_asyncio.apply()
            return asyncio.run(fetch_expireddomains_multi_pages(pages=4))
        else:
            return asyncio.run(fetch_expireddomains_multi_pages(pages=4))
            
    except RuntimeError:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            result = loop.run_until_complete(fetch_expireddomains_multi_pages(pages=4))
            return result
        finally:
            loop.close()


class DomainScanner:
    """åŸŸåæ‰«æå™¨ä¸»ç±»"""
    
    def __init__(self, mode='expireddomains'):
        self.mode = mode
    
    def scan(self) -> List[Dict]:
        """æ‰§è¡Œæ‰«æï¼ˆè¿”å› Top 5ï¼‰"""
        
        if self.mode == 'expireddomains':
            print("ğŸ•·ï¸ ä½¿ç”¨ ExpiredDomains.net çˆ¬è™«æ¨¡å¼ï¼ˆ4 é¡µ = 100 ä¸ªåŸŸåï¼‰")
            domains = fetch_from_expireddomains()
            
            if len(domains) == 0:
                print("âš ï¸ æœªæŠ“å–åˆ°åŸŸå")
                return []
            
            print(f"\nğŸ” å¼€å§‹è®¡ç®—è´¨é‡åˆ†æ•°ï¼ˆå…± {len(domains)} ä¸ªåŸŸåï¼‰...")
            return self._filter_high_quality(domains)
        
        else:
            print(f"âš ï¸ æœªçŸ¥æ¨¡å¼: {self.mode}")
            return []
    
    def _filter_high_quality(self, domains: List[Dict]) -> List[Dict]:
        """è®¡ç®—è´¨é‡åˆ†æ•°ï¼Œè¿”å› Top 5"""
        
        for domain in domains:
            score = 0
            
            # DA åˆ†æ•°æƒé‡ 30%ï¼ˆ0-100åˆ† â†’ 0-30ï¼‰
            score += domain.get('da_score', 0) * 0.3
            
            # å¤–é“¾æ•°é‡æƒé‡ 20%
            bl = domain.get('backlinks', 0)
            score += min(bl / 50, 20)  # 2500+ å¤–é“¾ = 20 åˆ†
            
            # å¼•ç”¨åŸŸæƒé‡ 20%
            rd = domain.get('referring_domains', 0)
            score += min(rd / 5, 20)  # 100+ å¼•ç”¨åŸŸ = 20 åˆ†
            
            # åŸŸåå¹´é¾„æƒé‡ 10%
            age = domain.get('domain_age', 0)
            score += min(age / 2, 10)  # 20+ å¹´ = 10 åˆ†
            
            # ç«ä»·ä»·æ ¼æƒé‡ 10%
            price = domain.get('price', 0)
            score += min(price / 200, 10)  # $2000+ = 10 åˆ†
            
            # ç«ä»·æ¬¡æ•°æƒé‡ 5%
            bids = domain.get('bids', 0)
            score += min(bids / 10, 5)  # 50+ æ¬¡ç«ä»· = 5 åˆ†
            
            # ç»´åŸºç™¾ç§‘å¤–é“¾æƒé‡ 5%
            wiki = domain.get('wikipedia_links', 0)
            score += min(wiki * 0.5, 5)  # 10+ ç»´åŸºé“¾æ¥ = 5 åˆ†
            
            domain['quality_score'] = round(score, 2)
        
        # æŒ‰è´¨é‡åˆ†æ•°æ’åºï¼ˆé™åºï¼‰
        domains.sort(key=lambda x: x.get('quality_score', 0), reverse=True)
        
        # æ‰“å° Top 5
        print(f"\n{'='*80}")
        print(f"ğŸ† TOP 5 é«˜è´¨é‡è¿‡æœŸåŸŸåï¼ˆå…±è¯„ä¼° {len(domains)} ä¸ªï¼‰")
        print(f"{'='*80}\n")
        
        for idx, d in enumerate(domains[:5], 1):
            print(f"{idx}. ã€{d['name']}ã€‘")
            print(f"   ğŸ“Š è´¨é‡åˆ†: {d.get('quality_score', 0):.1f}/100")
            print(f"   ğŸ”— DA: {d.get('da_score', 0)} | å¤–é“¾: {d.get('backlinks', 0):,} | å¼•ç”¨åŸŸ: {d.get('referring_domains', 0)}")
            print(f"   ğŸ“… å¹´é¾„: {d.get('domain_age', 0)}å¹´ | ä»·æ ¼: ${d.get('price', 0)} | ç«ä»·: {d.get('bids', 0)}æ¬¡ | Wiki: {d.get('wikipedia_links', 0)}")
            print()
        
        print(f"{'='*80}\n")
        
        # ğŸ”¥ åªè¿”å› Top 5
        return domains[:5]