import asyncio
import logging
import random
from typing import List, Dict, Optional
from datetime import datetime, timedelta
import whois
from curl_cffi.requests import AsyncSession
from bs4 import BeautifulSoup
from app.core.config import settings
from app.db.session import SessionLocal
from app.models.domain import Domain
from app.services.ai_generator import AIGenerator

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DomainScanner:
    def __init__(self):
        self.db = SessionLocal()
        self.ai_generator = AIGenerator()

    def verify_expiry_date_via_whois(self, domain_name: str) -> Dict:
        """
        éªŒè¯åŸŸåçš„çœŸå®åˆ°æœŸæ—¶é—´
        è¿”å›: {'real_expiry': datetime, 'is_expired': bool, 'is_valid': bool}
        """
        try:
            w = whois.whois(domain_name)
            
            # å¤„ç† whois è¿”å›çš„æ—¥æœŸå¯èƒ½æ˜¯åˆ—è¡¨çš„æƒ…å†µ
            expiry_date = w.expiration_date
            if isinstance(expiry_date, list):
                expiry_date = expiry_date[0]
                
            if not expiry_date:
                return {'real_expiry': None, 'is_expired': False, 'is_valid': False}
                
            now = datetime.now()
            # å¦‚æœåˆ°æœŸæ—¶é—´å°äºå½“å‰æ—¶é—´ï¼Œè¯´æ˜å·²è¿‡æœŸï¼ˆä¸”æœªç»­è´¹ï¼‰
            
            is_expired = expiry_date < now
            
            return {
                'real_expiry': expiry_date,
                'is_expired': is_expired,
                'is_valid': True
            }
        except Exception as e:
            # logger.error(f"WHOIS lookup failed for {domain_name}: {str(e)}")
            return {'real_expiry': None, 'is_expired': False, 'is_valid': False}

    async def fetch_single_page(self, page: int, retries=3) -> List[Dict]:
        """æŠ“å–å•é¡µæ•°æ®ï¼ˆå¸¦é‡è¯•ï¼‰"""
        url = "https://member.expireddomains.net/domains/expiredcom/"
        cookies = {
            "s_id": settings.EXPIRED_DOMAINS_COOKIE  # ä»ç¯å¢ƒå˜é‡è·å–
        }
        headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
            "Referer": "https://member.expireddomains.net/domains/expiredcom/"
        }

        # è®¡ç®—åˆ†é¡µå‚æ•°
        start = (page - 1) * 25
        params = {
            "start": start,
            "flimit": 25,
            "fwhois": "1",    # ä»…æ˜¾ç¤ºæœ‰ Whois çš„
            "fmarket": "0",   # æ’é™¤å¸‚åœºåŸŸå
            "flast24": "1"    # ä»…æœ€è¿‘ 24 å°æ—¶
        }

        for attempt in range(retries):
            try:
                # æ³¨æ„ï¼šcurl_cffi åœ¨æŸäº›ç¯å¢ƒä¸‹ close æ—¶ä¼šæŠ¥é”™ï¼Œè¿™é‡Œå°è¯•å¿½ç•¥
                try:
                    async with AsyncSession() as session:
                        resp = await session.get(url, params=params, cookies=cookies, headers=headers, timeout=30)
                        
                        if resp.status_code != 200:
                            logger.warning(f"Page {page} failed with status {resp.status_code}")
                            continue
                            
                        content = resp.text
                except RuntimeError:
                    # å¿½ç•¥ curl_cffi åœ¨å…³é—­ loop æ—¶çš„å·²çŸ¥é”™è¯¯
                    pass
                except Exception as e:
                    logger.error(f"Request error on page {page}: {e}")
                    continue

                domains = []
                soup = BeautifulSoup(content, 'html.parser')
                rows = soup.select('table.base1 tbody tr')
                
                for row in rows:
                    cols = row.find_all('td')
                    if len(cols) < 2:
                        continue
                        
                    domain_name = cols[0].get_text(strip=True)
                    
                    # æå–å…¶ä»–æŒ‡æ ‡ (ç¤ºä¾‹)
                    bl = 0 # Backlinks
                    try:
                        bl_text = cols[2].get_text(strip=True)
                        if 'K' in bl_text:
                            bl = int(float(bl_text.replace('K', '')) * 1000)
                        else:
                            bl = int(bl_text)
                    except:
                        pass

                    domains.append({
                        "domain": domain_name,
                        "source": "expireddomains.net",
                        "backlinks": bl,
                        "da_score": 0, # ç¨åè®¡ç®—
                        "status": "pending"
                    })
                
                logger.info(f"âœ… ç¬¬ {page} é¡µï¼šæˆåŠŸè§£æ {len(domains)} ä¸ªåŸŸå")
                return domains

            except Exception as e:
                logger.error(f"Attempt {attempt+1} failed for page {page}: {str(e)}")
                await asyncio.sleep(2)
        
        return []

    async def fetch_expireddomains_multi_pages(self, pages=4) -> List[Dict]:
        """å¹¶å‘æŠ“å–å¤šé¡µ"""
        logger.info(f"ğŸš€ å¼€å§‹æŠ“å–å‰ {pages} é¡µï¼ˆå…±çº¦ {pages*25} ä¸ªåŸŸåï¼‰...")
        tasks = [self.fetch_single_page(page) for page in range(1, pages + 1)]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        all_domains = []
        for res in results:
            if isinstance(res, list):
                all_domains.extend(res)
        
        logger.info(f"âœ… å…±æŠ“å– {len(all_domains)} ä¸ªåŸŸå")
        return all_domains

    def calculate_da_mock(self, domain: str) -> int:
        """æ¨¡æ‹Ÿè®¡ç®— DA åˆ†æ•° (è¿™é‡Œç”¨ç®€å•çš„ä¼ªéšæœºç®—æ³•ï¼Œå®é™…åº”è°ƒ API)"""
        # åŸºäºåŸŸåé•¿åº¦å’Œå­—ç¬¦åšç®€å•çš„å“ˆå¸Œæ˜ å°„ï¼Œä¿æŒåŒä¸€ä¸ªåŸŸååˆ†æ•°å›ºå®š
        seed = sum(ord(c) for c in domain)
        random.seed(seed)
        
        # 80% æ¦‚ç‡ä½åˆ†ï¼Œ20% æ¦‚ç‡é«˜åˆ†
        if random.random() > 0.8:
            return random.randint(20, 50)
        return random.randint(0, 15)

    def generate_mock_domains(self, count=20) -> List[Dict]:
        """Bå±‚ï¼šç”Ÿæˆæ¨¡æ‹Ÿçš„é«˜è´¨é‡åŸŸåï¼ˆé™çº§æ–¹æ¡ˆï¼‰"""
        logger.info(f"âš ï¸ [B å±‚] è§¦å‘é™çº§ï¼šç”Ÿæˆ {count} ä¸ªæ¨¡æ‹ŸåŸŸå")
        
        prefixes = ["cloud", "ai", "meta", "cyber", "tech", "data", "smart", "net", "web", "sys"]
        suffixes = ["hub", "lab", "box", "base", "now", "ify", "ly", "io", "dev", "app"]
        tlds = [".com", ".io", ".ai", ".net", ".org"]
        
        mock_domains = []
        for _ in range(count):
            d_name = f"{random.choice(prefixes)}{random.choice(suffixes)}{random.choice(tlds)}"
            mock_domains.append({
                "domain": d_name,
                "da_score": random.randint(15, 45),
                "backlinks": random.randint(100, 5000),
                "source": "mock_generator",
                "status": "available",
                "registered_at": datetime.now(),
                "expires_at": datetime.now() + timedelta(days=30)
            })
            
        return mock_domains

    async def generate_ai_domains(self, topic="technology", count=10) -> List[Dict]:
        """Cå±‚ï¼šè°ƒç”¨ AI ç”ŸæˆåŸŸåï¼ˆå¢å€¼æ–¹æ¡ˆï¼‰"""
        logger.info(f"ğŸ§  [C å±‚] è§¦å‘ AI ç”Ÿæˆï¼šä¸»é¢˜ {topic}, æ•°é‡ {count}")
        try:
            # è°ƒç”¨ AIGenerator æœåŠ¡
            ai_suggestions = await self.ai_generator.generate_domains(topic, count)
            
            formatted_domains = []
            for name in ai_suggestions:
                formatted_domains.append({
                    "domain": name,
                    "da_score": random.randint(25, 60), # AI ç”Ÿæˆçš„é€šå¸¸è´¨é‡è¾ƒé«˜
                    "backlinks": 0,
                    "source": "ai_claude",
                    "status": "suggestion",
                    "registered_at": datetime.now(),
                    "expires_at": datetime.now() + timedelta(days=365)
                })
            return formatted_domains
        except Exception as e:
            logger.error(f"AI generation failed: {e}")
            return []

    async def scan(self):
        """ä¸»æ‰«æé€»è¾‘ï¼šA -> B -> C é™çº§"""
        logger.info("ğŸš€ å¼€å§‹ä¸‰å±‚é™çº§æ‰«æ...")
        
        final_results = []
        
        # --- A å±‚ï¼šçœŸå®çˆ¬è™« ---
        logger.info("ğŸ•·ï¸ [A å±‚] æŠ“å– ExpiredDomains.net")
        raw_domains = await self.fetch_expireddomains_multi_pages(pages=4)
        
        # åªæœ‰å½“æŠ“å–åˆ°æ•°æ®æ—¶æ‰è¿›è¡ŒéªŒè¯
        if raw_domains:
            # 1. è®¡ç®—/è·å– DA åˆ†æ•°
            logger.info(f"ğŸ” å¼€å§‹è®¡ç®—è´¨é‡åˆ†æ•°ï¼ˆå…± {len(raw_domains)} ä¸ªåŸŸåï¼‰...")
            for d in raw_domains:
                d['da_score'] = self.calculate_da_mock(d['domain'])
                
            # 2. æŒ‰ DA æ’åºå– Top 5
            top_domains = sorted(raw_domains, key=lambda x: x['da_score'], reverse=True)[:5]
            
            # 3. WHOIS éªŒè¯
            logger.info("ğŸ” å¯¹ Top 5 è¿›è¡Œ WHOIS éªŒè¯...")
            valid_a_domains = []
            for d in top_domains:
                logger.info(f"Checking {d['domain']}...")
                verify_res = self.verify_expiry_date_via_whois(d['domain'])
                
                if verify_res['is_expired']:
                    logger.info(f"âœ… éªŒè¯é€šè¿‡: {d['domain']} (è¿‡æœŸæ—¥: {verify_res['real_expiry']})")
                    d['expires_at'] = verify_res['real_expiry']
                    d['status'] = 'expired_confirmed'
                    valid_a_domains.append(d)
                else:
                    logger.info(f"âŒ å·²ç»­è´¹: {d['domain']} (åˆ°æœŸæ—¥: {verify_res.get('real_expiry')})")
            
            final_results.extend(valid_a_domains)
        
        # --- B å±‚ï¼šæ¨¡æ‹Ÿæ•°æ®ï¼ˆå¦‚æœ A å±‚ç»“æœä¸è¶³ 2 ä¸ªï¼‰---
        if len(final_results) < 2:
            logger.info("âš ï¸ A å±‚æœ‰æ•ˆæ•°æ®ä¸è¶³ï¼Œå¯åŠ¨ B å±‚è¡¥ä½...")
            mock_data = self.generate_mock_domains(count=5 - len(final_results))
            final_results.extend(mock_data)
            
        # --- C å±‚ï¼šAI å¢å€¼ï¼ˆå¯é€‰ï¼Œæ€»æ˜¯è¡¥å……å‡ ä¸ªé«˜è´¨é‡å»ºè®®ï¼‰---
        # è¿™é‡Œå‡è®¾é…ç½®å¼€å¯ AI
        try:
            ai_data = await self.generate_ai_domains(topic="SaaS and AI", count=3)
            final_results.extend(ai_data)
        except Exception as e:
            logger.warning(f"C å±‚æ‰§è¡Œå¤±è´¥: {e}")

        # --- ç»“æœå…¥åº“ ---
        logger.info(f"ğŸ’¾ æ­£åœ¨ä¿å­˜ {len(final_results)} ä¸ªåŸŸååˆ°æ•°æ®åº“...")
        saved_count = 0
        for item in final_results:
            # æŸ¥é‡
            exists = self.db.query(Domain).filter(Domain.domain == item['domain']).first()
            if not exists:
                new_domain = Domain(
                    domain=item['domain'],
                    da_score=item.get('da_score', 0),
                    backlinks=item.get('backlinks', 0),
                    source=item.get('source', 'unknown'),
                    status=item.get('status', 'pending'),
                    expires_at=item.get('expires_at')
                )
                self.db.add(new_domain)
                saved_count += 1
        
        try:
            self.db.commit()
            logger.info(f"âœ… æˆåŠŸå…¥åº“ {saved_count} ä¸ªæ–°åŸŸå")
        except Exception as e:
            self.db.rollback()
            logger.error(f"æ•°æ®åº“æäº¤å¤±è´¥: {e}")

        return final_results