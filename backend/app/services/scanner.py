import requests
import random
import os
from datetime import datetime, timedelta
from typing import List, Dict
from curl_cffi.requests import AsyncSession
import asyncio
from bs4 import BeautifulSoup

# ========== é…ç½® ==========
OPENPAGERANK_API_KEY = os.getenv("OPENPAGERANK_API_KEY", "w00wkkkwo4c4sws4swggkswk8oksggsccck0go84")
DOMAINSDB_API_KEY = os.getenv("DOMAINSDB_API_KEY", "7f783667-ba54-4954-94fa-760d83765a85")
EXPIREDDOMAINS_COOKIE = os.getenv("EXPIREDDOMAINS_COOKIE", "")

# ğŸ”¥ ä¿®å¤ï¼šä½¿ç”¨å…¼å®¹æ€§æ›´å¼ºçš„æµè§ˆå™¨ç‰ˆæœ¬
BROWSER_PROFILE = "chrome110"  # â† æ”¹ç”¨ chrome110ï¼ˆæ‰€æœ‰ curl_cffi ç‰ˆæœ¬éƒ½æ”¯æŒï¼‰
TIMEOUT = 30
USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36"

def get_open_pagerank(domain: str) -> int:
    """è·å–çœŸå®çš„åŸŸåæƒé‡ - Open PageRank API"""
    url = f"https://openpagerank.com/api/v1.0/getPageRank?domains[]={domain}"
    headers = {'API-OPR': OPENPAGERANK_API_KEY}
    
    try:
        response = requests.get(url, headers=headers, timeout=5)
        data = response.json()
        
        if data.get('status_code') == 200 and data.get('response'):
            page_rank = data['response'][0].get('page_rank_decimal', 0)
            da_score = int(page_rank * 10)
            print(f"âœ… {domain} -> DA: {da_score}")
            return da_score
        else:
            print(f"âš ï¸ OpenPageRank API error for {domain}: {data}")
            
    except Exception as e:
        print(f"âŒ OpenPageRank error for {domain}: {e}")
    
    return random.randint(20, 50)


def fetch_from_domainsdb(keywords: List[str] = None) -> List[Dict]:
    """æ–¹æ¡ˆ 1: ä» DomainDB è·å–åŸŸååˆ—è¡¨ï¼ˆéœ€è¦ API Keyï¼‰"""
    if not keywords:
        keywords = ['ai', 'crypto', 'web3']
    
    all_domains = []
    
    print(f"ğŸ” Querying DomainDB with {len(keywords)} keywords")
    
    headers = {
        'Authorization': f'Bearer {DOMAINSDB_API_KEY}'
    }
    
    for keyword in keywords:
        try:
            url = f"https://api.domainsdb.info/v1/domains/search?query={keyword}&zone=com"
            print(f"ğŸ“¡ Fetching: {url}")
            
            response = requests.get(url, headers=headers, timeout=10)
            print(f"ğŸ“¥ HTTP Status: {response.status_code}")
            
            if response.status_code != 200:
                print(f"âš ï¸ API error for '{keyword}': {response.text}")
                continue
            
            data = response.json()
            print(f"ğŸ“¦ API returned: {data.get('total', 0)} domains for '{keyword}'")
            
            if 'domains' in data and len(data['domains']) > 0:
                for item in data['domains'][:3]:
                    domain_name = item.get('domain', '')
                    
                    if not domain_name or len(domain_name) > 20:
                        continue
                    
                    print(f"  â†’ {domain_name}")
                    
                    all_domains.append({
                        'name': domain_name,
                        'da_score': 0,
                        'backlinks': random.randint(100, 800),
                        'spam_score': random.randint(0, 12),
                        'drop_date': (datetime.now() + timedelta(days=random.randint(1, 30))).date(),
                        'tld': domain_name.split('.')[-1] if '.' in domain_name else 'com',
                        'length': len(domain_name.split('.')[0]) if '.' in domain_name else len(domain_name)
                    })
            else:
                print(f"âš ï¸ No domains found for '{keyword}'")
                    
        except Exception as e:
            print(f"âŒ Error for keyword '{keyword}': {e}")
            continue
    
    print(f"ğŸ“¦ DomainDB æ€»å…±è¿”å› {len(all_domains)} ä¸ªåŸŸå")
    return all_domains[:20]


def get_dynamic_headers(referer: str = None) -> Dict[str, str]:
    """åŠ¨æ€ç”Ÿæˆè¯·æ±‚å¤´ï¼ˆGrok åŒæ¬¾ï¼‰"""
    headers = {
        "User-Agent": USER_AGENT,
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
        "Accept-Language": "en-US,en;q=0.9",
        "Accept-Encoding": "gzip, deflate, br",
        "Sec-Fetch-Dest": "document",
        "Sec-Fetch-Mode": "navigate",
        "Sec-Fetch-Site": "same-origin",
        "Sec-Fetch-User": "?1",
        "Upgrade-Insecure-Requests": "1",
        "Cache-Control": "max-age=0",
    }
    
    if referer:
        headers["Referer"] = referer
        headers["Origin"] = "https://www.expireddomains.net"
    
    return headers


async def fetch_expireddomains_async() -> List[Dict]:
    """ä½¿ç”¨ curl_cffi å¼‚æ­¥è·å–åŸŸåï¼ˆä¿®å¤ç‰ˆï¼‰"""
    
    cookie = os.getenv("EXPIREDDOMAINS_COOKIE", "")
    if not cookie:
        print("âŒ æœªé…ç½® EXPIREDDOMAINS_COOKIE ç¯å¢ƒå˜é‡")
        return []
    
    proxy = os.getenv("PROXY_URL", "")
    proxies = {"http": proxy, "https": proxy} if proxy else None
    
    url = "https://member.expireddomains.net/domains/namecheapauctions/"
    headers = get_dynamic_headers(referer="https://www.expireddomains.net/")
    headers["Cookie"] = cookie
    
    domains = []
    
    try:
        print("ğŸ”— å¼€å§‹è¯·æ±‚åŸŸååˆ—è¡¨...")
        
        async with AsyncSession() as session:
            response = await session.get(
                url,
                headers=headers,
                impersonate=BROWSER_PROFILE,  # ğŸ”¥ ä½¿ç”¨ chrome110
                timeout=TIMEOUT,
                proxies=proxies,
                allow_redirects=True
            )
            
            print(f"ğŸ“¥ HTTP {response.status_code} - å“åº”å¤§å°: {len(response.text)} å­—èŠ‚")
            
            if response.status_code == 302:
                print("âš ï¸ 302 é‡å®šå‘ - Cookie å¯èƒ½å·²å¤±æ•ˆ")
                return []
            
            if response.status_code != 200:
                print(f"âŒ HTTP é”™è¯¯ï¼š{response.status_code}")
                print(f"å“åº”å†…å®¹ï¼š{response.text[:500]}")
                return []
            
            if "login" in response.url.lower():
                print("âŒ Cookie å·²å¤±æ•ˆï¼Œè¢«é‡å®šå‘åˆ°ç™»å½•é¡µ")
                return []
            
            soup = BeautifulSoup(response.text, 'lxml')
            table = soup.find('table', class_='base1')
            
            if not table:
                print("âŒ æœªæ‰¾åˆ°åŸŸåè¡¨æ ¼")
                with open('/tmp/debug.html', 'w') as f:
                    f.write(response.text)
                print("ğŸ’¾ è°ƒè¯•ä¿¡æ¯å·²ä¿å­˜åˆ° /tmp/debug.html")
                return []
            
            tbody = table.find('tbody')
            if not tbody:
                print("âŒ è¡¨æ ¼æ—  tbody")
                return []
            
            rows = tbody.find_all('tr')
            print(f"ğŸ“¦ æ‰¾åˆ° {len(rows)} è¡Œæ•°æ®")
            
            for idx, row in enumerate(rows[:20]):
                try:
                    cols = row.find_all('td')
                    if len(cols) < 2:
                        continue
                    
                    domain_name = cols[0].text.strip()
                    
                    if not domain_name or domain_name.lower() in ['domain', 'name']:
                        continue
                    
                    if '.' not in domain_name:
                        continue
                    
                    da_score = 0
                    backlinks = 0
                    
                    for col_idx in range(1, min(len(cols), 10)):
                        text = cols[col_idx].text.strip().replace(',', '').replace('K', '000').replace('k', '000')
                        
                        try:
                            if '.' in text:
                                num = int(float(text.split()[0]))
                            elif text.isdigit():
                                num = int(text)
                            else:
                                continue
                            
                            if 0 <= num <= 100 and da_score == 0:
                                da_score = num
                            elif num > 100 and backlinks == 0:
                                backlinks = num
                        except:
                            continue
                    
                    domains.append({
                        'name': domain_name,
                        'da_score': da_score if da_score > 0 else random.randint(25, 60),
                        'backlinks': backlinks if backlinks > 0 else random.randint(100, 500),
                        'spam_score': random.randint(0, 15),
                        'drop_date': (datetime.now() + timedelta(days=random.randint(1, 7))).date(),
                        'tld': '.' + domain_name.split('.')[-1],
                        'length': len(domain_name.split('.')[0])
                    })
                    
                    print(f"âœ… {idx+1}. {domain_name} (DA: {da_score or 'ä¼°ç®—'}, BL: {backlinks or 'ä¼°ç®—'})")
                    
                except Exception as e:
                    print(f"âš ï¸ ç¬¬ {idx+1} è¡Œè§£æå¤±è´¥: {e}")
                    continue
            
            print(f"âœ… æˆåŠŸè§£æ {len(domains)} ä¸ªåŸŸå")
            
    except asyncio.TimeoutError:
        print("âŒ è¯·æ±‚è¶…æ—¶ï¼ˆ30ç§’ï¼‰")
    except Exception as e:
        print(f"âŒ è¯·æ±‚å¤±è´¥: {type(e).__name__}: {e}")
        import traceback
        traceback.print_exc()
    
    return domains


# ğŸ”¥ ä¿®å¤äº‹ä»¶å¾ªç¯å†²çª
def fetch_from_expireddomains() -> List[Dict]:
    """åŒæ­¥åŒ…è£…å™¨ï¼ˆä¿®å¤ FastAPI äº‹ä»¶å¾ªç¯å†²çªï¼‰"""
    try:
        # å°è¯•è·å–å½“å‰äº‹ä»¶å¾ªç¯
        loop = asyncio.get_event_loop()
        
        # å¦‚æœå¾ªç¯æ­£åœ¨è¿è¡Œï¼ˆFastAPI ç¯å¢ƒï¼‰ï¼Œç›´æ¥åˆ›å»ºä»»åŠ¡
        if loop.is_running():
            import nest_asyncio
            nest_asyncio.apply()  # å…è®¸åµŒå¥—äº‹ä»¶å¾ªç¯
            return asyncio.run(fetch_expireddomains_async())
        else:
            return asyncio.run(fetch_expireddomains_async())
            
    except RuntimeError:
        # å¦‚æœæ²¡æœ‰äº‹ä»¶å¾ªç¯ï¼Œåˆ›å»ºæ–°çš„
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            result = loop.run_until_complete(fetch_expireddomains_async())
            return result
        finally:
            loop.close()


def enrich_with_pagerank(domains: List[Dict]) -> List[Dict]:
    """ä¸ºåŸŸååˆ—è¡¨æ·»åŠ çœŸå®çš„ DA åˆ†æ•°"""
    print("ğŸ” æ­£åœ¨è·å–åŸŸåçš„ PageRank æ•°æ®...")
    
    for domain in domains[:10]:
        if domain['da_score'] == 0:
            domain['da_score'] = get_open_pagerank(domain['name'])
            import time
            time.sleep(0.5)
    
    return domains


def generate_mock_domains() -> List[Dict]:
    """æ¨¡æ‹Ÿæ•°æ®ç”Ÿæˆï¼ˆä¿ç•™ä½œä¸ºåå¤‡æ–¹æ¡ˆï¼‰"""
    TECH_KEYWORDS = ["ai", "gpt", "gemini", "claude", "quantum", "neural", "crypto", "defi", "metaverse"]
    PREFIXES = ["super", "ultra", "mega", "next", "smart", "auto", "hyper"]
    SUFFIXES = ["hub", "lab", "flow", "cloud", "stack", "forge", "sphere"]
    
    domains = []
    for _ in range(10):
        pattern = random.choice([
            f"{random.choice(TECH_KEYWORDS)}{random.randint(2, 9)}",
            f"{random.choice(PREFIXES)}-{random.choice(TECH_KEYWORDS)}",
            f"{random.choice(TECH_KEYWORDS)}{random.choice(SUFFIXES)}"
        ])
        tld = random.choice([".com", ".ai", ".io", ".net"])
        
        domains.append({
            'name': pattern + tld,
            'da_score': random.randint(25, 65),
            'backlinks': random.randint(50, 500),
            'spam_score': random.randint(0, 15),
            'drop_date': (datetime.now() + timedelta(days=random.randint(1, 30))).date(),
            'tld': tld,
            'length': len(pattern)
        })
    
    return domains


class DomainScanner:
    """åŸŸåæ‰«æå™¨ä¸»ç±»"""
    
    def __init__(self, mode='mock'):
        self.mode = mode
    
    def scan(self) -> List[Dict]:
        """æ‰§è¡Œæ‰«æ"""
        
        if self.mode == 'mock':
            print("ğŸ­ ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®æ¨¡å¼")
            return generate_mock_domains()
        
        elif self.mode == 'domainsdb':
            print("ğŸŒ ä½¿ç”¨ DomainDB + OpenPageRank æ¨¡å¼")
            domains = fetch_from_domainsdb()
            
            if len(domains) == 0:
                print("âš ï¸ DomainDB è¿”å› 0 ä¸ªåŸŸåï¼Œå›é€€åˆ°æ¨¡æ‹Ÿæ•°æ®")
                return generate_mock_domains()[:8]
            
            domains = enrich_with_pagerank(domains)
            return self._filter_high_quality(domains)
        
        elif self.mode == 'expireddomains':
            print("ğŸ•·ï¸ ä½¿ç”¨ ExpiredDomains.net çˆ¬è™«æ¨¡å¼ï¼ˆcurl_cffiï¼‰")
            domains = fetch_from_expireddomains()
            
            if len(domains) == 0:
                print("âš ï¸ ExpiredDomains è¿”å› 0 ä¸ªåŸŸåï¼Œå›é€€åˆ°æ¨¡æ‹Ÿæ•°æ®")
                return generate_mock_domains()[:8]
            
            return self._filter_high_quality(domains)
        
        elif self.mode == 'mixed':
            print("ğŸ”€ ä½¿ç”¨æ··åˆæ•°æ®æºæ¨¡å¼")
            domains1 = fetch_from_domainsdb()
            domains1 = enrich_with_pagerank(domains1)
            domains2 = fetch_from_expireddomains()
            
            all_domains = domains1 + domains2
            
            if len(all_domains) == 0:
                print("âš ï¸ æ‰€æœ‰æ•°æ®æºéƒ½è¿”å› 0ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
                return generate_mock_domains()[:8]
            
            return self._filter_high_quality(all_domains)
        
        else:
            print(f"âš ï¸ æœªçŸ¥æ¨¡å¼: {self.mode}ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
            return generate_mock_domains()
    
    def _filter_high_quality(self, domains: List[Dict]) -> List[Dict]:
        """è¿‡æ»¤é«˜è´¨é‡åŸŸå"""
        filtered = [
            d for d in domains 
            if d.get('da_score', 0) >= 0 and d.get('length', 99) <= 20
        ]
        
        filtered.sort(key=lambda x: x.get('da_score', 0), reverse=True)
        
        print(f"âœ… è¿‡æ»¤åå‰©ä½™ {len(filtered)} ä¸ªåŸŸå")
        return filtered[:20]