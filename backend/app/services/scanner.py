import requests
import random
import os
import re
from datetime import datetime, timedelta
from typing import List, Dict
from curl_cffi.requests import AsyncSession
import asyncio
from bs4 import BeautifulSoup
import time
import whois
from anthropic import Anthropic
import google.generativeai as genai

from app.core.config import settings

OPENPAGERANK_API_KEY = os.getenv("OPENPAGERANK_API_KEY", "w00wkkkwo4c4sws4swggkswk8oksggsccck0go84")
EXPIREDDOMAINS_COOKIE = os.getenv("EXPIREDDOMAINS_COOKIE", "")

BROWSER_PROFILE = "chrome110"
TIMEOUT = 30
USER_AGENT = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36"


# ============= æ–°å¢ï¼šWHOIS éªŒè¯å‡½æ•° =============
def verify_expiry_date_via_whois(domain_name: str) -> Dict[str, any]:
    """
    é€šè¿‡ WHOIS æŸ¥è¯¢éªŒè¯çœŸå®åˆ°æœŸæ—¥æœŸ
    
    è¿”å›ï¼š
    {
        'domain': 'example.com',
        'real_expiry': datetime(2026, 11, 5),
        'is_expired': False,
        'is_available': False,
        'error': None
    }
    """
    try:
        w = whois.whois(domain_name)
        
        expiry_date = w.expiration_date
        
        if isinstance(expiry_date, list):
            expiry_date = expiry_date[0]
        
        if not expiry_date:
            return {
                'domain': domain_name,
                'real_expiry': None,
                'is_expired': False,
                'is_available': False,
                'error': 'No expiry date found'
            }
        
        today = datetime.now()
        is_expired = expiry_date < today
        
        grace_period = timedelta(days=30)
        is_available = (today - expiry_date) > grace_period
        
        return {
            'domain': domain_name,
            'real_expiry': expiry_date,
            'is_expired': is_expired,
            'is_available': is_available,
            'error': None
        }
        
    except Exception as e:
        return {
            'domain': domain_name,
            'real_expiry': None,
            'is_expired': False,
            'is_available': False,
            'error': str(e)
        }


# ============= æ–°å¢ï¼šMock å•è¯åº“ï¼ˆæ‰©å±•ç‰ˆï¼‰=============
WORD_POOL = [
    # AI/ç§‘æŠ€ç±»
    'ai', 'cloud', 'neural', 'deep', 'bot', 'auto', 'smart', 'quantum',
    'cyber', 'data', 'algo', 'crypto', 'meta', 'chain', 'edge', 'sync',
    'neural', 'tensor', 'vector', 'matrix',
    
    # åŠ¨ä½œç±»
    'build', 'forge', 'craft', 'make', 'grow', 'scale', 'flow', 'link',
    'hub', 'lab', 'base', 'core', 'hive', 'mesh', 'grid', 'nexus',
    'create', 'launch', 'spark', 'boost',
    
    # ä¸šåŠ¡ç±»
    'saas', 'api', 'app', 'dev', 'ops', 'tool', 'kit', 'suite', 'stack',
    'platform', 'studio', 'space', 'zone', 'spot', 'dash', 'pulse',
    'work', 'task', 'team', 'crew',
    
    # è¡Œä¸šç±»
    'health', 'finance', 'edu', 'legal', 'retail', 'media', 'travel',
    'music', 'sport', 'game', 'book', 'food', 'fashion', 'home',
    'tech', 'code', 'design', 'market',
    
    # å½¢å®¹è¯
    'fast', 'easy', 'simple', 'quick', 'instant', 'magic', 'super',
    'pro', 'max', 'ultra', 'prime', 'elite', 'plus', 'next', 'neo',
    'swift', 'rapid', 'agile', 'smart',
    
    # åè¯
    'sky', 'ocean', 'mountain', 'river', 'forest', 'star', 'moon',
    'sun', 'earth', 'wind', 'fire', 'light', 'stone', 'gold', 'silver',
    'wave', 'beam', 'spark', 'flux'
]

TLD_POOL = ['.ai', '.io', '.dev', '.app', '.tech', '.cloud', '.co', '.me']


def generate_mock_domains(count: int = 20) -> List[Dict]:
    """ç”Ÿæˆéšæœºç»„åˆåŸŸåï¼ˆæ‰©å±•ç‰ˆï¼‰"""
    domains = []
    
    for _ in range(count):
        word_count = random.choice([2, 3])
        words = random.sample(WORD_POOL, word_count)
        name = ''.join(words)
        
        tld = random.choice(TLD_POOL)
        domain_name = f"{name}{tld}"
        
        domains.append({
            'name': domain_name,
            'da_score': random.randint(5, 25),
            'backlinks': random.randint(100, 1000),
            'referring_domains': random.randint(10, 100),
            'spam_score': random.randint(0, 20),
            'drop_date': (datetime.now() + timedelta(days=random.randint(1, 30))).date(),
            'tld': tld,
            'length': len(name),
            'domain_age': random.randint(1, 5),
            'price': random.randint(10, 200),
            'bids': random.randint(0, 5),
            'wikipedia_links': random.randint(0, 3),
            'quality_score': 0.0
        })
    
    return domains


# ============= æ–°å¢ï¼šAI ç”ŸæˆåŸŸå =============
def generate_ai_domains(topic: str = "AI tools", count: int = 20) -> List[Dict]:
    """é€šè¿‡ AI ç”Ÿæˆé«˜è´¨é‡åŸŸåå»ºè®®ï¼ˆæ”¯æŒ Claude å’Œ Geminiï¼‰"""
    
    provider = settings.ai_provider.lower()
    
    # ===== Claude ç”Ÿæˆ =====
    if provider == "claude":
        if not settings.anthropic_api_key:
            print("âš ï¸ æœªé…ç½® ANTHROPIC_API_KEYï¼Œè·³è¿‡ AI ç”Ÿæˆ")
            return []
        
        try:
            client = Anthropic(api_key=settings.anthropic_api_key)
            
            prompt = f"""Generate {count} premium domain name suggestions for: "{topic}".

Requirements:
1. Short (5-15 chars before TLD)
2. Memorable, pronounceable
3. Related to {topic}
4. Use .ai, .io, .dev, .app, .tech, .cloud

Output format (one per line):
domainname.tld

Examples:
- cloudforge.ai
- buildhub.io

Generate {count} domains (only names, no explanations):"""

            message = client.messages.create(
                model=settings.ai_model_claude,
                max_tokens=1000,
                messages=[{"role": "user", "content": prompt}]
            )
            
            content = message.content[0].text.strip()
            domain_lines = [line.strip() for line in content.split('\n') if '.' in line]
            
            print(f"âœ… Claude ç”Ÿæˆ {len(domain_lines[:count])} ä¸ªåŸŸå")
            return _parse_ai_domains(domain_lines[:count])
            
        except Exception as e:
            print(f"âŒ Claude ç”Ÿæˆå¤±è´¥: {e}")
            return []
    
    # ===== Gemini ç”Ÿæˆ =====
    elif provider == "gemini":
        if not settings.google_api_key:
            print("âš ï¸ æœªé…ç½® GOOGLE_API_KEYï¼Œè·³è¿‡ AI ç”Ÿæˆ")
            return []
        
        try:
            genai.configure(api_key=settings.google_api_key)
            model = genai.GenerativeModel(settings.ai_model_gemini)
            
            prompt = f"""Generate {count} premium domain name suggestions for: "{topic}".

Requirements:
1. Short (5-15 chars before TLD)
2. Memorable, pronounceable
3. Related to {topic}
4. Use .ai, .io, .dev, .app, .tech, .cloud

Output format (one per line):
domainname.tld

Examples:
- cloudforge.ai
- buildhub.io

Generate {count} domains (only names, no explanations):"""

            response = model.generate_content(prompt)
            content = response.text.strip()
            domain_lines = [line.strip() for line in content.split('\n') if '.' in line]
            
            print(f"âœ… Gemini ç”Ÿæˆ {len(domain_lines[:count])} ä¸ªåŸŸå")
            return _parse_ai_domains(domain_lines[:count])
            
        except Exception as e:
            print(f"âŒ Gemini ç”Ÿæˆå¤±è´¥: {e}")
            return []
    
    else:
        print(f"âš ï¸ æœªçŸ¥ AI æä¾›å•†: {provider}")
        return []


def _parse_ai_domains(domain_lines: List[str]) -> List[Dict]:
    """è§£æ AI è¿”å›çš„åŸŸååˆ—è¡¨"""
    domains = []
    
    for domain_name in domain_lines:
        domain_name = domain_name.split('. ', 1)[-1].strip()
        domain_name = domain_name.lstrip('- ')
        
        if not domain_name or '.' not in domain_name:
            continue
        
        tld = '.' + domain_name.split('.')[-1]
        name_part = domain_name.split('.')[0]
        
        domains.append({
            'name': domain_name,
            'da_score': 0,
            'backlinks': 0,
            'referring_domains': 0,
            'spam_score': 0,
            'drop_date': (datetime.now() + timedelta(days=7)).date(),
            'tld': tld,
            'length': len(name_part),
            'domain_age': 0,
            'price': 0,
            'bids': 0,
            'wikipedia_links': 0,
            'quality_score': 0.0
        })
    
    return domains


# ============= åŸæœ‰å‡½æ•°ä¿æŒä¸å˜ =============
def extract_number(text: str) -> int:
    """æ­£åˆ™æå–æ•°å­—ï¼Œå¤„ç† 1.8Kã€1,992 ç­‰æ ¼å¼"""
    if not text:
        return 0
    
    match = re.search(r'(\d+(?:\.\d+)?)\s*K', text.upper())
    if match:
        return int(float(match.group(1)) * 1000)
    
    match = re.search(r'(\d[\d,]*)', text)
    if match:
        return int(match.group(1).replace(',', ''))
    
    return 0


def batch_get_pagerank(domain_names: List[str]) -> Dict[str, int]:
    """æ‰¹é‡è·å– DA åˆ†æ•°ï¼ˆä¸€æ¬¡æœ€å¤š 100 ä¸ªåŸŸåï¼Œé¿å… API è¶…é™ï¼‰"""
    
    if not domain_names:
        return {}
    
    results = {}
    batch_size = 100
    
    print(f"ğŸ” å¼€å§‹æ‰¹é‡è·å– {len(domain_names)} ä¸ªåŸŸåçš„ DA åˆ†æ•°...")
    
    for i in range(0, len(domain_names), batch_size):
        batch = domain_names[i:i+batch_size]
        
        params = {f"domains[{j}]": domain for j, domain in enumerate(batch)}
        
        try:
            response = requests.get(
                "https://openpagerank.com/api/v1.0/getPageRank",
                params=params,
                headers={'API-OPR': OPENPAGERANK_API_KEY},
                timeout=10
            )
            
            data = response.json()
            
            if data.get('status_code') == 200 and data.get('response'):
                for item in data['response']:
                    domain = item['domain']
                    page_rank = item.get('page_rank_decimal', 0)
                    da_score = int(page_rank * 10)
                    results[domain] = da_score
                    print(f"  âœ… {domain} â†’ DA: {da_score}")
                
                print(f"âœ… æ‰¹æ¬¡å®Œæˆ: æˆåŠŸè·å– {len(batch)} ä¸ªåŸŸåçš„ DA")
            else:
                print(f"âš ï¸ OpenPageRank API é”™è¯¯: {data}")
                for domain in batch:
                    results[domain] = 0
            
            time.sleep(1)
            
        except Exception as e:
            print(f"âŒ æ‰¹é‡è·å– DA å¤±è´¥: {e}")
            for domain in batch:
                results[domain] = 0
    
    return results


def get_dynamic_headers(referer: str = None) -> Dict[str, str]:
    """åŠ¨æ€ç”Ÿæˆè¯·æ±‚å¤´"""
    headers = {
        "User-Agent": USER_AGENT,
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
        "Accept-Language": "en-US,en;q=0.9",
        "Accept-Encoding": "gzip, deflate, br",
        "Sec-Fetch-Dest": "document",
        "Sec-Fetch-Mode": "navigate",
        "Sec-Fetch-Site": "same-origin",
        "Sec-Fetch-User": "?1",
        "Upgrade-Insecure-Requests": "1",
        "Cache-Control": "max-age=0",
    }
    
    if referer:
        headers["Referer"] = referer
        headers["Origin"] = "https://www.expireddomains.net"
    
    return headers


async def fetch_single_page(start: int = 0) -> List[Dict]:
    """æŠ“å–å•é¡µæ•°æ®ï¼ˆ25 ä¸ªåŸŸåï¼‰"""
    
    cookie = os.getenv("EXPIREDDOMAINS_COOKIE", "")
    if not cookie:
        print("âŒ æœªé…ç½® EXPIREDDOMAINS_COOKIE ç¯å¢ƒå˜é‡")
        return []
    
    proxy = os.getenv("PROXY_URL", "")
    proxies = {"http": proxy, "https": proxy} if proxy else None
    
    if start == 0:
        url = "https://member.expireddomains.net/domains/namecheapauctions/?start=0#listing"
    else:
        url = f"https://member.expireddomains.net/domains/namecheapauctions/?start={start}#listing"
    
    headers = get_dynamic_headers(referer="https://www.expireddomains.net/")
    headers["Cookie"] = cookie
    
    domains = []
    page_num = start // 25 + 1
    
    try:
        print(f"ğŸ”— æ­£åœ¨æŠ“å–ç¬¬ {page_num} é¡µ (start={start})...")
        
        async with AsyncSession() as session:
            response = await session.get(
                url,
                headers=headers,
                impersonate=BROWSER_PROFILE,
                timeout=TIMEOUT,
                proxies=proxies,
                allow_redirects=True
            )
            
            if response.status_code != 200:
                print(f"âŒ HTTP é”™è¯¯ï¼š{response.status_code}")
                return []
            
            if "login" in response.url.lower():
                print("âŒ Cookie å·²å¤±æ•ˆï¼Œè¢«é‡å®šå‘åˆ°ç™»å½•é¡µ")
                return []
            
            soup = BeautifulSoup(response.text, 'lxml')
            table = soup.find('table', class_='base1')
            
            if not table:
                print(f"âŒ ç¬¬ {page_num} é¡µæœªæ‰¾åˆ°åŸŸåè¡¨æ ¼")
                return []
            
            tbody = table.find('tbody')
            if not tbody:
                print(f"âŒ ç¬¬ {page_num} é¡µè¡¨æ ¼æ—  tbody")
                return []
            
            rows = tbody.find_all('tr')
            
            for idx, row in enumerate(rows):
                try:
                    cols = row.find_all('td')
                    if len(cols) < 23:
                        continue
                    
                    domain_name = cols[0].text.strip()
                    
                    if not domain_name or domain_name.lower() in ['domain', 'name']:
                        continue
                    
                    if '.' not in domain_name:
                        continue
                    
                    backlinks = extract_number(cols[4].text.strip())
                    referring_domains = extract_number(cols[5].text.strip())
                    
                    wby_text = cols[6].text.strip()
                    try:
                        domain_age_year = int(wby_text) if wby_text.isdigit() else 0
                    except:
                        domain_age_year = 0
                    
                    age_years = (datetime.now().year - domain_age_year) if domain_age_year > 1900 else 0
                    
                    wikipedia_links = extract_number(cols[20].text.strip()) if len(cols) > 20 else 0
                    price = extract_number(cols[21].text.strip()) if len(cols) > 21 else 0
                    bids = extract_number(cols[22].text.strip()) if len(cols) > 22 else 0
                    
                    domains.append({
                        'name': domain_name,
                        'da_score': 0,
                        'backlinks': backlinks,
                        'referring_domains': referring_domains,
                        'spam_score': random.randint(0, 15),
                        'drop_date': (datetime.now() + timedelta(days=random.randint(1, 7))).date(),
                        'tld': '.' + domain_name.split('.')[-1],
                        'length': len(domain_name.split('.')[0]),
                        'domain_age': age_years,
                        'price': price,
                        'bids': bids,
                        'wikipedia_links': wikipedia_links
                    })
                    
                except Exception as e:
                    print(f"âš ï¸ ç¬¬ {page_num} é¡µç¬¬ {idx+1} è¡Œè§£æå¤±è´¥: {e}")
                    continue
            
            print(f"âœ… ç¬¬ {page_num} é¡µï¼šæˆåŠŸè§£æ {len(domains)} ä¸ªåŸŸå")
            
    except Exception as e:
        print(f"âŒ ç¬¬ {page_num} é¡µæŠ“å–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    
    await asyncio.sleep(2)
    
    return domains


async def fetch_expireddomains_multi_pages(pages: int = 4) -> List[Dict]:
    """æŠ“å–å‰ N é¡µï¼ˆé»˜è®¤ 4 é¡µ = 100 ä¸ªåŸŸåï¼Œé¿å… API è¶…é™ï¼‰"""
    
    all_domains = []
    
    print(f"ğŸš€ å¼€å§‹æŠ“å–å‰ {pages} é¡µï¼ˆå…±çº¦ {pages * 25} ä¸ªåŸŸåï¼‰...")
    
    for page_num in range(pages):
        start = page_num * 25
        domains = await fetch_single_page(start)
        all_domains.extend(domains)
        
        if len(domains) == 0:
            print(f"âš ï¸ ç¬¬ {page_num + 1} é¡µæ— æ•°æ®ï¼Œåœæ­¢æŠ“å–")
            break
    
    print(f"\nâœ… å…±æŠ“å– {len(all_domains)} ä¸ªåŸŸå")
    
    if all_domains:
        domain_names = [d['name'] for d in all_domains]
        da_scores = batch_get_pagerank(domain_names)
        
        for domain in all_domains:
            domain['da_score'] = da_scores.get(domain['name'], 0)
    
    return all_domains


def fetch_from_expireddomains() -> List[Dict]:
    """åŒæ­¥åŒ…è£…å™¨ï¼ˆæŠ“å– 4 é¡µ = 100 ä¸ªåŸŸåï¼‰"""
    return asyncio.run(fetch_expireddomains_multi_pages(pages=4))


# ============= ä¿®æ”¹ï¼šä¸»æ‰«æå™¨ç±»ï¼ˆä¸‰å±‚é™çº§ï¼‰ =============
class DomainScanner:
    """åŸŸåæ‰«æå™¨ä¸»ç±»"""
    
    def __init__(self, mode='expireddomains'):
        self.mode = mode
    
    def scan(self) -> List[Dict]:
        """ä¸‰å±‚é™çº§æ‰«æ"""
        
        print("\n" + "="*80)
        print("ğŸš€ å¼€å§‹ä¸‰å±‚é™çº§æ‰«æ...")
        print("="*80 + "\n")
        
        domains = []
        
        # ===== A å±‚ï¼šçœŸå®çˆ¬è™« + WHOIS éªŒè¯ =====
        if self.mode == 'expireddomains':
            print("ğŸ•·ï¸ [A å±‚] æŠ“å– ExpiredDomains.netï¼ˆ4 é¡µ = 100 ä¸ªåŸŸåï¼‰")
            raw_domains = fetch_from_expireddomains()
            
            if raw_domains:
                print(f"\nğŸ” å¼€å§‹ WHOIS éªŒè¯ï¼ˆå…± {len(raw_domains)} ä¸ªåŸŸåï¼‰...\n")
                
                verified_domains = []
                
                for idx, domain_data in enumerate(raw_domains, 1):
                    domain_name = domain_data['name']
                    
                    print(f"  [{idx}/{len(raw_domains)}] éªŒè¯ {domain_name}...")
                    
                    whois_result = verify_expiry_date_via_whois(domain_name)
                    
                    if whois_result['error']:
                        print(f"    âš ï¸ WHOIS æŸ¥è¯¢å¤±è´¥: {whois_result['error']}")
                        continue
                    
                    if not whois_result['is_expired']:
                        real_expiry = whois_result['real_expiry']
                        print(f"    âŒ å·²ç»­è´¹ï¼šçœŸå®åˆ°æœŸæ—¥æœŸ {real_expiry.strftime('%Y-%m-%d')}")
                        continue
                    
                    if whois_result['is_available']:
                        print(f"    âœ… çœŸæ­£è¿‡æœŸå¯æ³¨å†Œ")
                        domain_data['drop_date'] = whois_result['real_expiry'].date()
                        verified_domains.append(domain_data)
                    else:
                        print(f"    â³ åœ¨å®½é™æœŸå†…")
                
                domains.extend(verified_domains)
                print(f"\nâœ… [A å±‚] éªŒè¯åå‰©ä½™ {len(verified_domains)} ä¸ªçœŸæ­£è¿‡æœŸçš„åŸŸå\n")
            
            else:
                print("âŒ [A å±‚] çˆ¬è™«å¤±è´¥ï¼Œè¿›å…¥é™çº§æ¨¡å¼\n")
        
        # ===== B å±‚ï¼šMock ç»„åˆåŸŸåï¼ˆé™çº§ï¼‰ =====
        if len(domains) < 5:
            print("ğŸ”„ [B å±‚] ç”Ÿæˆç»„åˆåŸŸåï¼ˆé™çº§å…œåº•ï¼‰")
            mock_domains = generate_mock_domains(count=20)
            domains.extend(mock_domains)
            print(f"âœ… [B å±‚] ç”Ÿæˆ {len(mock_domains)} ä¸ªç»„åˆåŸŸå\n")
        
        # ===== C å±‚ï¼šAI ç”Ÿæˆï¼ˆå¯é€‰ï¼‰ =====
        if len(domains) < 5 and (settings.anthropic_api_key or settings.google_api_key):
            print("ğŸ¤– [C å±‚] AI ç”Ÿæˆé«˜è´¨é‡åŸŸåï¼ˆæœ€ç»ˆå…œåº•ï¼‰")
            ai_domains = generate_ai_domains(topic="SaaS and AI tools", count=20)
            domains.extend(ai_domains)
            print(f"âœ… [C å±‚] AI ç”Ÿæˆ {len(ai_domains)} ä¸ªåŸŸå\n")
        
        # ===== è®¡ç®—è´¨é‡åˆ†æ•° + è¿”å› Top 5 =====
        if not domains:
            print("âŒ ä¸‰å±‚æ‰«æå…¨éƒ¨å¤±è´¥")
            return []
        
        print(f"ğŸ” å¼€å§‹è®¡ç®—è´¨é‡åˆ†æ•°ï¼ˆå…± {len(domains)} ä¸ªåŸŸåï¼‰...\n")
        return self._filter_high_quality(domains)
    
    def _filter_high_quality(self, domains: List[Dict]) -> List[Dict]:
        """è®¡ç®—è´¨é‡åˆ†æ•°ï¼Œè¿”å› Top 5"""
        
        for domain in domains:
            score = 0
            
            score += domain.get('da_score', 0) * 0.3
            
            bl = domain.get('backlinks', 0)
            score += min(bl / 50, 20)
            
            rd = domain.get('referring_domains', 0)
            score += min(rd / 5, 20)
            
            age = domain.get('domain_age', 0)
            score += min(age / 2, 10)
            
            price = domain.get('price', 0)
            score += min(price / 200, 10)
            
            bids = domain.get('bids', 0)
            score += min(bids / 10, 5)
            
            wiki = domain.get('wikipedia_links', 0)
            score += min(wiki * 0.5, 5)
            
            domain['quality_score'] = round(score, 2)
        
        domains.sort(key=lambda x: x.get('quality_score', 0), reverse=True)
        
        print(f"\n{'='*80}")
        print(f"ğŸ† TOP 5 é«˜è´¨é‡è¿‡æœŸåŸŸåï¼ˆå…±è¯„ä¼° {len(domains)} ä¸ªï¼‰")
        print(f"{'='*80}\n")
        
        for idx, d in enumerate(domains[:5], 1):
            print(f"{idx}. ã€{d['name']}ã€‘")
            print(f"   ğŸ“Š è´¨é‡åˆ†: {d.get('quality_score', 0):.1f}/100")
            print(f"   ğŸ”— DA: {d.get('da_score', 0)} | å¤–é“¾: {d.get('backlinks', 0):,} | å¼•ç”¨åŸŸ: {d.get('referring_domains', 0)}")
            print(f"   ğŸ“… å¹´é¾„: {d.get('domain_age', 0)}å¹´ | ä»·æ ¼: ${d.get('price', 0)} | ç«ä»·: {d.get('bids', 0)}æ¬¡ | Wiki: {d.get('wikipedia_links', 0)}")
            print()
        
        print(f"{'='*80}\n")
        
        return domains[:5]